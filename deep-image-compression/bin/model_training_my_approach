# Run this script at directory deep-image-compression/, with command:
# '''bin/model_training_my_approach'''

# Train my approach model (improved version of Balle2018) on
# professional_train dataset from CLIC

# LAMBDA need to be adjusted for different target compression ratio

# TRAIN_BATCH_SIZE, MAIN_LEARNING_RATE, AUX_LEARNING_RATE can be adjusted to
# optimize model performance

# Compared with Balle2018, the LAMBDA need to be scale to 2X to achieve
# similar target bpp, since my approach has one more max pooling compared to
# Balle2018

TRAIN_DATA_PATH=../data/raw/professional_train
MODEL_PATH="./model/my_approach"
NUM_FILTERS=192
LAMBDA=0.02
PREPROCESS_THREADS=16
TRAIN_BATCH_SIZE=8
MAX_TRAIN_STEPS=1000000
MAIN_LEARNING_RATE=1e-4
AUX_LEARNING_RATE=1e-3
export CUDA_VISIBLE_DEVICES=0

python3 my_approach.py \
--verbose \
--num_filters ${NUM_FILTERS} \
--checkpoint_dir ${MODEL_PATH} \
train \
--train_glob "${TRAIN_DATA_PATH}/*.png" \
--lambda ${LAMBDA} \
--preprocess_threads ${PREPROCESS_THREADS} \
--batchsize ${TRAIN_BATCH_SIZE} \
--last_step ${MAX_TRAIN_STEPS} \
--main_learning_rate ${MAIN_LEARNING_RATE} \
--aux_learning_rate ${AUX_LEARNING_RATE}
