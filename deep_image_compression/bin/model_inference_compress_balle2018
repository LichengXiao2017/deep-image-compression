# Run this script at directory deep-image-compression/, with command:
# '''bin/model_inference_compress_balle2018'''

# Inference Balle2018 model and compress image
# Running time will be around 2X (~ 8 seconds per image on GTX 1070) compared
# with offcial evaluation script Using pretrained model, because building model
# from scratch and then restoring parameters from local checkpoint takes longer
# than restoring model from gzip file directly.

TEST_DATA_PATH=/path/to/test/image.png
MODEL_PATH="./model/balle2018"
NUM_FILTERS=192

python3 balle2018.py \
--num_filters ${NUM_FILTERS} \
--checkpoint_dir ${MODEL_PATH} \
compress \
--input_file ${TEST_DATA_PATH}
